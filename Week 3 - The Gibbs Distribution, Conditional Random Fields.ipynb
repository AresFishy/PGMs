{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a more general form than pairwise Markov Models\n",
    "\n",
    "A fully connected pairwise Markov Network with $n$ variables, each with $d$ values, has $O(n^2d^2)$ parameters. However, a general distribution has $O(d^n)$ parameters, which is much much larger. Thus, **pairwise Markov networks cannot express any probability distribution**\n",
    "\n",
    "### Enter the Gibbs Distribution\n",
    "\n",
    "Instead of factors over pairs, we now have *general factors* over pairs, triplets, quadruplets etc.: i.e. we have factors over every subset of factors in the model (not necessarily all possible relationships).\n",
    "\n",
    "(Remember, a full probability distribution is a factor whose Scope is $X_1 \\text{ to } X_n$)\n",
    "\n",
    "A Gibbs distribution is parameterized by a set of factors $\\Phi$:\n",
    "\n",
    "$$ \\Phi = \\{\\phi_1(D_1), \\dotsc , \\phi_k(D_k) \\}$$\n",
    "\n",
    "With factor product\n",
    "\n",
    "$$ \\tilde{P_{\\phi}}(X_1, \\dotsc , X_n) = \\prod_{i-1}^k \\phi_i(D_i)$$\n",
    "\n",
    "Again, this is not necessarily a probability distribution and needs a partition function $Z_{\\Phi}$\n",
    "\n",
    "**What Markov Network do we want for a full Gibbs distribution?**\n",
    "\n",
    "<img src=\"imgs/induced_mn.png\" width=80%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **induced Markov Network** $H_{\\phi}$ has an edge $X_i - X_j$ whenever there exists $\\phi_{ij} \\in \\Phi \\text{ such that } x_i, x_j \\in \\overline{D_m}$, where $\\overline{D_m}$ is the scope.\n",
    "\n",
    "We cannot read the factorization of an induced MN from the graph: **different factorizations with different expressive powers induce the same graph**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Trails in Markov Networks\n",
    "\n",
    "A trail $X_1 - \\dotsc -X_n$ is *active* given a set of observed variables $Z$ if no $X_i$ is in $Z$\n",
    "\n",
    "i.e. we can only have a trail of influence over unobserved variables. This is in contrast to a Bayesian Network, where if $Z$ is the descendant of a v-structure, it creates an active trail allowing influence to flow between its parents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Random Fields\n",
    "\n",
    "More commonly used than other Markov Network types. It is intended to deal with task-specific prediction, when we have input variables x and target variables y.\n",
    "\n",
    "e.g. **Image Segmentation: **\n",
    "\n",
    "X are pixel values & processed pixel features, and Y is a class for every pixel\n",
    "\n",
    "**Text processing:** Words in sentence -> Labels of words (e.g. person, location etc.)\n",
    "\n",
    "In a Naive Bayes model where the features are independent given the labels, we have a lot of redundancy since the features are generally correlated. Thus, if we have many copies of the same feature, we count it multiple times and our prediction is dominated by that feature. \n",
    "\n",
    "It is hard to add edges to capture the correlations and gives rise to densely connected models. \n",
    "\n",
    "The CRF Representation **avoids these problems by modeling $P(Y \\ | \\ X)$** and not the joint distribution $P(Y,X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRF looks a lot like a Gibbs distribution, except that the unnormalized measure $\\tilde{P}$ is $\\tilde{P_{\\phi}}(X,Y)$ rather than over the variables $X_1, \\dotsc , X_n$.\n",
    "\n",
    "We therefore need a partition function that is a function of $X$:\n",
    "\n",
    "$$Z_{\\phi}(X) = \\sum_Y \\tilde{P_{\\phi}}(X,Y)$$\n",
    "\n",
    "In essence this is the marginal for each instance of $X$.\n",
    "\n",
    "Thus, the whole expression is: \n",
    "\n",
    "$$P_{\\phi}(Y \\ | \\ X) = \\frac{1}{Z_{\\phi}(X)} \\tilde{P_{\\phi}}(X,Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRFs and the Logistic Model\n",
    "\n",
    "With **binary** features and labels, we can express the factor $\\phi_i$ as\n",
    "\n",
    "$$\\phi_i(X_i,Y) = \\exp\\{w_i\\mathbf{1}\\{X_i = 1, Y = 1\\}\\}$$, \n",
    "\n",
    "where $\\mathbf{1}$ is the *indicator function* and is valued 1 when $X_i = Y$ and is 0 otherwise.\n",
    "\n",
    "Plugging this into the CRF:\n",
    "\n",
    "$$\\phi_i(X_i,Y=1) = \\exp\\{w_ix_i\\}$$,\n",
    "\n",
    "since in this case $\\mathbf{1}$ is 1 if $X_i=1$, and 0 vice versa.\n",
    "\n",
    "Thus our unnormalized density is \n",
    "\n",
    "$$P_{\\phi}(X, Y=1) =  \\exp\\{\\sum_i(w_iX_i)\\}$$\n",
    "\n",
    "Quite neatly, our Conditional Distribution is then:\n",
    "\n",
    "$$P_{\\phi}(Y=1 \\ | \\ X) = \\frac{\\exp\\{\\sum_i(w_iX_i)\\}}{1 + \\exp\\{\\sum_i(w_iX_i)\\}}$$\n",
    "\n",
    "which, as we've learned by now, is the logistic function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
